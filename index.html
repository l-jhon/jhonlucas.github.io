<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_5a502iysdq90-2{list-style-type:none}ul.lst-kix_5a502iysdq90-1{list-style-type:none}ul.lst-kix_5a502iysdq90-4{list-style-type:none}ul.lst-kix_5a502iysdq90-3{list-style-type:none}.lst-kix_le5gy5easr5s-0>li:before{content:"\0025cf   "}ul.lst-kix_5a502iysdq90-0{list-style-type:none}.lst-kix_le5gy5easr5s-1>li:before{content:"\0025cb   "}.lst-kix_2o2zx8ep6cda-6>li:before{content:"\0025cf   "}.lst-kix_2o2zx8ep6cda-7>li:before{content:"\0025cb   "}ul.lst-kix_o57yxehwqq42-8{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-8{list-style-type:none}ul.lst-kix_o57yxehwqq42-7{list-style-type:none}ul.lst-kix_o57yxehwqq42-6{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-6{list-style-type:none}ul.lst-kix_o57yxehwqq42-5{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-7{list-style-type:none}ul.lst-kix_o57yxehwqq42-4{list-style-type:none}.lst-kix_2o2zx8ep6cda-8>li:before{content:"\0025a0   "}ul.lst-kix_c4cd17rhy6dz-4{list-style-type:none}ul.lst-kix_o57yxehwqq42-3{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-5{list-style-type:none}ul.lst-kix_o57yxehwqq42-2{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-2{list-style-type:none}ul.lst-kix_o57yxehwqq42-1{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-3{list-style-type:none}ul.lst-kix_o57yxehwqq42-0{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-0{list-style-type:none}ul.lst-kix_c4cd17rhy6dz-1{list-style-type:none}ul.lst-kix_xlu6ijxiwzdn-0{list-style-type:none}.lst-kix_c4cd17rhy6dz-6>li:before{content:"\0025cf   "}.lst-kix_5a502iysdq90-5>li:before{content:"\0025a0   "}.lst-kix_5a502iysdq90-6>li:before{content:"\0025cf   "}.lst-kix_c4cd17rhy6dz-5>li:before{content:"\0025a0   "}.lst-kix_c4cd17rhy6dz-7>li:before{content:"\0025cb   "}ul.lst-kix_xlu6ijxiwzdn-4{list-style-type:none}.lst-kix_c4cd17rhy6dz-4>li:before{content:"\0025cb   "}.lst-kix_c4cd17rhy6dz-8>li:before{content:"\0025a0   "}ul.lst-kix_xlu6ijxiwzdn-3{list-style-type:none}.lst-kix_5a502iysdq90-3>li:before{content:"\0025cf   "}.lst-kix_5a502iysdq90-7>li:before{content:"\0025cb   "}ul.lst-kix_xlu6ijxiwzdn-2{list-style-type:none}ul.lst-kix_xlu6ijxiwzdn-1{list-style-type:none}ul.lst-kix_xlu6ijxiwzdn-8{list-style-type:none}ul.lst-kix_149c1l7vjrop-2{list-style-type:none}ul.lst-kix_xlu6ijxiwzdn-7{list-style-type:none}ul.lst-kix_149c1l7vjrop-1{list-style-type:none}.lst-kix_5a502iysdq90-1>li:before{content:"\0025cb   "}.lst-kix_5a502iysdq90-2>li:before{content:"\0025a0   "}ul.lst-kix_xlu6ijxiwzdn-6{list-style-type:none}ul.lst-kix_149c1l7vjrop-4{list-style-type:none}ul.lst-kix_xlu6ijxiwzdn-5{list-style-type:none}ul.lst-kix_149c1l7vjrop-3{list-style-type:none}ul.lst-kix_149c1l7vjrop-6{list-style-type:none}ul.lst-kix_149c1l7vjrop-5{list-style-type:none}.lst-kix_5a502iysdq90-0>li:before{content:"\0025cf   "}.lst-kix_5a502iysdq90-8>li:before{content:"\0025a0   "}ul.lst-kix_149c1l7vjrop-8{list-style-type:none}ul.lst-kix_149c1l7vjrop-7{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-5{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-6{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-3{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-4{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-1{list-style-type:none}.lst-kix_c4cd17rhy6dz-0>li:before{content:"\0025cf   "}ul.lst-kix_2o2zx8ep6cda-2{list-style-type:none}ul.lst-kix_149c1l7vjrop-0{list-style-type:none}.lst-kix_5kbtd9y4gx42-8>li:before{content:"\0025a0   "}ul.lst-kix_6w3ktdc8zhvk-8{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-0{list-style-type:none}ul.lst-kix_6w3ktdc8zhvk-7{list-style-type:none}.lst-kix_5kbtd9y4gx42-7>li:before{content:"\0025cb   "}.lst-kix_c4cd17rhy6dz-2>li:before{content:"\0025a0   "}ul.lst-kix_6w3ktdc8zhvk-6{list-style-type:none}ul.lst-kix_6w3ktdc8zhvk-5{list-style-type:none}.lst-kix_c4cd17rhy6dz-1>li:before{content:"\0025cb   "}.lst-kix_c4cd17rhy6dz-3>li:before{content:"\0025cf   "}ul.lst-kix_6w3ktdc8zhvk-4{list-style-type:none}ul.lst-kix_6w3ktdc8zhvk-3{list-style-type:none}.lst-kix_5kbtd9y4gx42-5>li:before{content:"\0025a0   "}ul.lst-kix_6w3ktdc8zhvk-2{list-style-type:none}.lst-kix_5a502iysdq90-4>li:before{content:"\0025cb   "}ul.lst-kix_6w3ktdc8zhvk-1{list-style-type:none}.lst-kix_5kbtd9y4gx42-6>li:before{content:"\0025cf   "}ul.lst-kix_6w3ktdc8zhvk-0{list-style-type:none}.lst-kix_5kbtd9y4gx42-3>li:before{content:"\0025cf   "}.lst-kix_5kbtd9y4gx42-0>li:before{content:"\0025cf   "}.lst-kix_5kbtd9y4gx42-4>li:before{content:"\0025cb   "}.lst-kix_5kbtd9y4gx42-1>li:before{content:"\0025cb   "}.lst-kix_5kbtd9y4gx42-2>li:before{content:"\0025a0   "}ul.lst-kix_2o2zx8ep6cda-7{list-style-type:none}ul.lst-kix_2o2zx8ep6cda-8{list-style-type:none}.lst-kix_2o2zx8ep6cda-5>li:before{content:"\0025a0   "}.lst-kix_2o2zx8ep6cda-4>li:before{content:"\0025cb   "}.lst-kix_2o2zx8ep6cda-3>li:before{content:"\0025cf   "}.lst-kix_2o2zx8ep6cda-1>li:before{content:"\0025cb   "}.lst-kix_2o2zx8ep6cda-0>li:before{content:"\0025cf   "}.lst-kix_2o2zx8ep6cda-2>li:before{content:"\0025a0   "}ul.lst-kix_5kbtd9y4gx42-0{list-style-type:none}ul.lst-kix_5kbtd9y4gx42-1{list-style-type:none}.lst-kix_o57yxehwqq42-0>li:before{content:"\0025cf   "}.lst-kix_o57yxehwqq42-2>li:before{content:"\0025a0   "}ul.lst-kix_5kbtd9y4gx42-8{list-style-type:none}.lst-kix_o57yxehwqq42-1>li:before{content:"\0025cb   "}.lst-kix_o57yxehwqq42-3>li:before{content:"\0025cf   "}ul.lst-kix_5kbtd9y4gx42-6{list-style-type:none}ul.lst-kix_5kbtd9y4gx42-7{list-style-type:none}ul.lst-kix_5kbtd9y4gx42-4{list-style-type:none}ul.lst-kix_5kbtd9y4gx42-5{list-style-type:none}ul.lst-kix_5kbtd9y4gx42-2{list-style-type:none}ul.lst-kix_5kbtd9y4gx42-3{list-style-type:none}.lst-kix_o57yxehwqq42-6>li:before{content:"\0025cf   "}.lst-kix_o57yxehwqq42-5>li:before{content:"\0025a0   "}.lst-kix_o57yxehwqq42-7>li:before{content:"\0025cb   "}.lst-kix_o57yxehwqq42-4>li:before{content:"\0025cb   "}.lst-kix_o57yxehwqq42-8>li:before{content:"\0025a0   "}.lst-kix_6w3ktdc8zhvk-8>li:before{content:"\0025a0   "}ul.lst-kix_le5gy5easr5s-1{list-style-type:none}ul.lst-kix_le5gy5easr5s-2{list-style-type:none}ul.lst-kix_le5gy5easr5s-3{list-style-type:none}ul.lst-kix_le5gy5easr5s-4{list-style-type:none}ul.lst-kix_le5gy5easr5s-5{list-style-type:none}.lst-kix_6w3ktdc8zhvk-6>li:before{content:"\0025cf   "}ul.lst-kix_le5gy5easr5s-6{list-style-type:none}ul.lst-kix_le5gy5easr5s-7{list-style-type:none}ul.lst-kix_le5gy5easr5s-8{list-style-type:none}.lst-kix_6w3ktdc8zhvk-7>li:before{content:"\0025cb   "}.lst-kix_6w3ktdc8zhvk-0>li:before{content:"\0025cf   "}.lst-kix_6w3ktdc8zhvk-1>li:before{content:"\0025cb   "}.lst-kix_6w3ktdc8zhvk-2>li:before{content:"\0025a0   "}.lst-kix_6w3ktdc8zhvk-4>li:before{content:"\0025cb   "}.lst-kix_6w3ktdc8zhvk-5>li:before{content:"\0025a0   "}.lst-kix_xlu6ijxiwzdn-0>li:before{content:"\0025cf   "}.lst-kix_6w3ktdc8zhvk-3>li:before{content:"\0025cf   "}.lst-kix_xlu6ijxiwzdn-3>li:before{content:"\0025cf   "}.lst-kix_xlu6ijxiwzdn-4>li:before{content:"\0025cb   "}.lst-kix_xlu6ijxiwzdn-1>li:before{content:"\0025cb   "}.lst-kix_xlu6ijxiwzdn-2>li:before{content:"\0025a0   "}.lst-kix_xlu6ijxiwzdn-5>li:before{content:"\0025a0   "}.lst-kix_xlu6ijxiwzdn-6>li:before{content:"\0025cf   "}.lst-kix_149c1l7vjrop-0>li:before{content:"\0025cf   "}.lst-kix_le5gy5easr5s-7>li:before{content:"\0025cb   "}.lst-kix_149c1l7vjrop-1>li:before{content:"\0025cb   "}.lst-kix_149c1l7vjrop-2>li:before{content:"\0025a0   "}.lst-kix_le5gy5easr5s-8>li:before{content:"\0025a0   "}.lst-kix_149c1l7vjrop-3>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_le5gy5easr5s-2>li:before{content:"\0025a0   "}.lst-kix_149c1l7vjrop-5>li:before{content:"\0025a0   "}.lst-kix_149c1l7vjrop-6>li:before{content:"\0025cf   "}ul.lst-kix_5a502iysdq90-6{list-style-type:none}.lst-kix_149c1l7vjrop-4>li:before{content:"\0025cb   "}ul.lst-kix_5a502iysdq90-5{list-style-type:none}.lst-kix_149c1l7vjrop-8>li:before{content:"\0025a0   "}ul.lst-kix_5a502iysdq90-8{list-style-type:none}.lst-kix_le5gy5easr5s-3>li:before{content:"\0025cf   "}ul.lst-kix_5a502iysdq90-7{list-style-type:none}.lst-kix_le5gy5easr5s-6>li:before{content:"\0025cf   "}.lst-kix_xlu6ijxiwzdn-7>li:before{content:"\0025cb   "}.lst-kix_xlu6ijxiwzdn-8>li:before{content:"\0025a0   "}.lst-kix_le5gy5easr5s-4>li:before{content:"\0025cb   "}.lst-kix_149c1l7vjrop-7>li:before{content:"\0025cb   "}.lst-kix_le5gy5easr5s-5>li:before{content:"\0025a0   "}ul.lst-kix_le5gy5easr5s-0{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c1{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c9{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c18{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c19{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c17{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c11{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c12{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c14{background-color:#ffffff;max-width:453.5pt;padding:85pt 56.7pt 56.7pt 85pt}.c16{margin-left:72pt;padding-left:0pt}.c7{color:inherit;text-decoration:inherit}.c3{padding:0;margin:0}.c6{font-weight:700}.c13{height:20pt}.c4{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c14 doc-content"><p class="c19 title" id="h.i6boo048zhjx"><span class="c8">Data Engineering - Portfolio</span></p><p class="c17 subtitle" id="h.xakmbdrdu9k6"><span class="c18">Jhon Lucas - Data Engineer</span></p><h1 class="c11" id="h.53wz7u2ogq7z"><span class="c5">About me</span></h1><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Master&#39;s student in Computer Science, working on research on the application of NLP techniques to assist in the translation of sentences into Libras (Brazilian Sign Language), B.Sc. in Computer Science, and postgraduate in Big Data and Machine Learning. I have worked in the technology field for over eight years and currently work as a Data Engineering Specialist at Spectral Finance (https://www.spectral.finance). I have experience as a data platform Chapter Lead, supporting data engineers with guidelines, templates/frameworks, and best practices, focusing on ETL processes, data integration between different sources, Data Lake, Data Warehouse, Data Architecture, and Infrastructure. Experience with technology used at Moderna Data Stack like Apache Spark, Apache Hudi, Delta Lake, Airbyte, Meltano, and orchestration platforms like Apache Airflow, using these technologies primarily on the AWS Cloud. Enthusiastic about Machine Learning and Deep Learning. Co-founder of the Data Train Community and AWS Community Builder.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span>To show my portfolio I&#39;m going to use the </span><span class="c6">STAR</span><span class="c0">&nbsp;method, it&#39;s a method that I think is great for covering all the details of the project.</span></p><h1 class="c11" id="h.6zwrpy9pklh0"><span class="c5">Projects</span></h1><h2 class="c9" id="h.nxzmrzanjgns"><span class="c10">BI Platform - Retail</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: I was working on a project that involved defining business rules with the business team. We also needed to develop the data infrastructure necessary for the project&#39;s success. To accomplish these goals, we enlisted the help of a consulting company to guide us in defining the data modeling required for our Data Warehouse (DW).</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My primary responsibilities included cooperating with the business team to define the business rules and assisting with the deployment of services necessary for the project. Additionally, I needed to work with the consulting company to establish the data modeling for the DW, which included defining fact and dimension tables.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: To execute my tasks, I collaborated closely with the business team, translating their needs into concrete business rules. I assisted in deploying the necessary services for the project&#39;s infrastructure, ensuring it would be capable of handling the requirements of our work. Furthermore, I actively participated in the discussions and design process with the consulting company to effectively define the data modeling for our DW. This process involved determining the layout of our fact and dimension tables and using schema data modeling for our data warehouse.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: Through our joint efforts, we successfully defined the business rules and established a robust data infrastructure for the project. By collaborating with the consulting company, we also effectively determined the data modeling necessary for our DW, which now includes well-structured fact and dimension tables. These achievements have not only led to the successful realization of the project&#39;s objectives but also enhanced our data management capabilities.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Tech Stack:</span></p><ul class="c3 lst-kix_6w3ktdc8zhvk-0 start"><li class="c1 li-bullet-0"><span class="c0">Microsoft SQL Server (DW)</span></li><li class="c1 li-bullet-0"><span class="c0">Microsoft BI Stack</span></li></ul><ul class="c3 lst-kix_6w3ktdc8zhvk-1 start"><li class="c2 c16 li-bullet-0"><span class="c0">SQL Server Analytic Services (SSAS)</span></li><li class="c2 c16 li-bullet-0"><span class="c0">SQL Server Reporting Services (SSRS)</span></li><li class="c2 c16 li-bullet-0"><span class="c0">SQL Server Integration Services (SSIS)</span></li></ul><ul class="c3 lst-kix_6w3ktdc8zhvk-0"><li class="c1 li-bullet-0"><span class="c0">On-premise computer resource (Own Data Center)</span></li><li class="c1 li-bullet-0"><span class="c0">Targit (Data Visualization Tool)</span></li><li class="c1 li-bullet-0"><span class="c0">SQL to apply the transformations and data modeling</span></li></ul><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 245.33px;"><img alt="" src="images/image7.png" style="width: 604.72px; height: 245.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">With this project, we were able to start making decisions based on data, initiating a data-driven culture within the company. This shift was significant; we no longer relied on gut feelings for decision-making. Additionally, it helped us understand how we could improve both our business strategies and marketing campaigns. This, in turn, led to increased revenue and, consequently, profit.</span></p><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.nuyx70fhpcde"><span class="c10">Algorithm to reduce Stockout - Retail </span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: Our company was facing a high number of product stockouts across all branches.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to develop an algorithm that could suggest product transfers between branches with the aim of reducing stockouts. This task had to be completed based on business rules defined by the business area. Additionally, I was to create a consolidated data view on stockouts to be consumed in a Data Visualization tool for increased dynamism.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: To tackle the situation, I first carried out an exploratory analysis to understand the nuances of the stockouts across all branches. With a thorough understanding of the business rules, I developed an algorithm capable of suggesting product transfers between branches. The algorithm was designed to balance stocks and significantly reduce instances of stockouts. Simultaneously, I worked on the creation of a consolidated data view or a &quot;Big Table&quot;, which incorporated all information regarding stockouts. This table was specifically structured for compatibility with our Data Visualization tool, which facilitated a dynamic representation of the data.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: As a result of implementing the algorithm, we experienced a noticeable decrease in product stockouts across all branches. This strategic reallocation of products based on data-driven decisions substantially improved our inventory management. Additionally, the creation of the consolidated data view facilitated easy consumption of stockout data in a more visually appealing and accessible format, enhancing our ability to quickly understand and react to stockout patterns.&#39;</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Tech Stack:</span></p><ul class="c3 lst-kix_o57yxehwqq42-0 start"><li class="c1 li-bullet-0"><span class="c0">On-premise</span></li><li class="c1 li-bullet-0"><span class="c0">Oracle Database (Data Source)</span></li><li class="c1 li-bullet-0"><span class="c0">Python</span></li><li class="c1 li-bullet-0"><span class="c0">SQL</span></li><li class="c1 li-bullet-0"><span class="c0">Matplotlib</span></li><li class="c1 li-bullet-0"><span class="c0">Jupyter Notebook</span></li></ul><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.jq8qmvx2932l"><span class="c10">RFM (recency, frequency, monetary) Analysis</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: Our company needed an effective approach to customer segmentation that would improve our marketing efficiency and sales outcomes.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to employ RFM (Recency, Frequency, Monetary Value) analysis for customer segmentation and to conduct cluster analysis on our customer database. This segmentation was intended to better target our marketing efforts and subsequently boost sales.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I began by collecting and preparing all the necessary customer data for the RFM analysis. Based on the customers&#39; most recent purchases (Recency), purchase frequency (Frequency), and the total money spent (Monetary Value), I scored each customer and assigned them to various segments. After completing the RFM segmentation, I further carried out cluster analysis to group customers with similar buying behaviors. These groups allowed us to tailor our marketing strategies to each segment&#39;s unique characteristics and needs.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: The application of RFM segmentation and cluster analysis to our customer base led to notable improvements in our marketing efforts. Our marketing team was able to make more targeted advertisements, resulting in more effective campaigns and a significant increase in sales. The customer segmentation provided a clearer understanding of our customer base, enabling us to better serve their needs and boost our company&#39;s performance.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 133.33px;"><img alt="" src="images/image11.png" style="width: 604.72px; height: 133.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Tech Stack:</span></p><ul class="c3 lst-kix_5kbtd9y4gx42-0 start"><li class="c1 li-bullet-0"><span class="c0">On-premise</span></li><li class="c1 li-bullet-0"><span class="c0">Oracle Database</span></li><li class="c1 li-bullet-0"><span class="c0">SQL</span></li><li class="c1 li-bullet-0"><span class="c0">Python</span></li><li class="c1 li-bullet-0"><span class="c0">Metabase (Data Viz Tool)</span></li></ul><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.aonntzhgm9af"><span class="c10">Data Lakehouse Platform - Marketplace</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: Our company sought to become a data-driven organization. We identified the need to construct a data lakehouse platform on AWS, which would enable the business to access crucial data for decision-making and insights. This project involved handling more than 30 databases, over 300 tables, and storing more than 35TB of data on S3. Also in this project, we were handling third-party data sources, like rest API and GrapQL.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to contribute to the creation of this data lakehouse platform. This work involved utilizing a range of technologies, including Spark on EMR with Hudi as the table format, Athena, ECS to run Python scripts for retrieving data from Rest APIs, Airbyte for additional data extraction from Rest APIs, Airflow, S3, and Power BI on Azure for BI analysis.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I began by setting up Spark on AWS EMR and choosing Hudi as our table format to handle our large and complex data ecosystem. I then configured Athena to enable interactive queries on the data. To gather data from Rest APIs, I implemented Python scripts on ECS and used Airbyte. For orchestrating and scheduling our tasks, I incorporated Airflow into our tech stack. The data collected was stored in S3, and Power BI on Azure was used to facilitate BI analysis, providing a more digestible view of the data. I worked closely with my team to ensure that the process was seamless and that the data was accurately represented and easy to access.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: The successful completion of the data lakehouse platform marked a significant milestone for our company. We were not only able to handle a vast amount of data effectively but also made it accessible for business insights and decision-making purposes. The platform has empowered our company to become a truly data-driven organization, as the readily available data now plays a critical role in our business strategies. As a result, we&#39;ve seen improvements in our decision-making process, which has led to more targeted strategies and better business outcomes.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 300.00px;"><img alt="" src="images/image16.png" style="width: 604.72px; height: 300.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Tech Stack:</span></p><ul class="c3 lst-kix_2o2zx8ep6cda-0 start"><li class="c1 li-bullet-0"><span class="c0">AWS Cloud</span></li><li class="c1 li-bullet-0"><span class="c0">AWS EMR to run Pyspark Scripts</span></li><li class="c1 li-bullet-0"><span class="c0">Airflow (AWS MWAA)</span></li><li class="c1 li-bullet-0"><span class="c0">S3</span></li><li class="c1 li-bullet-0"><span class="c0">AWS Athena (Query Engine)</span></li><li class="c1 li-bullet-0"><span class="c0">Power BI</span></li><li class="c1 li-bullet-0"><span class="c0">AWS ECS</span></li><li class="c1 li-bullet-0"><span class="c0">Hudi (Open Data Lake Format)</span></li><li class="c1 li-bullet-0"><span class="c0">Airbyte (Integration Tool)</span></li><li class="c1 li-bullet-0"><span class="c0">AWS RDS</span></li><li class="c1 li-bullet-0"><span class="c0">Grafana (Monitoring)</span></li><li class="c1 li-bullet-0"><span class="c0">Terraform</span></li></ul><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.4la37l4v5qtu"><span class="c10">Data Ingestion Pipeline - From DynamoDB to Data Lakehouse</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: Our marketplace platform allows clients to rapidly publish their products via API and retrieve information about their products. However, our business team wanted to summarize the event data generated in DynamoDB to perform certain analyses.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to design and implement a pipeline that could extract data from DynamoDB and load it into our Data Lakehouse, making it accessible for analysis.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: To achieve this goal, I leveraged AWS Kinesis Streams to capture the real-time data from DynamoDB. I used Kinesis Analytics to process the data streams, extracting the useful insights that our business team needed. Then, I stored this data in S3, which acts as our Data Lakehouse. Finally, to make this data accessible and easily analyzable, I employed Athena, a service that enables users to perform SQL queries directly on data stored in S3.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: The implementation of this pipeline significantly enhanced our data accessibility and usability. The business team can now summarize and analyze event data directly from DynamoDB, leading to more data-driven decisions and improved business strategies. By using real-time data processing tools, we ensured that the data is always up-to-date, providing an accurate representation of the current state of affairs. The application of Athena has also made data querying a lot simpler, allowing users to focus on deriving insights rather than data extraction.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 474.64px; height: 333.02px;"><img alt="" src="images/image1.png" style="width: 474.64px; height: 333.02px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">&nbsp;</span></p><h2 class="c9" id="h.7tpxq2f9zje6"><span class="c10">Webhook to listen to CRM Events and insert them into Data Lake</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: Our Sales team needed real-time insights from CRM events to make informed decisions. However, we lacked a system that could listen to CRM events and provide these insights instantaneously.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to create a webhook to listen to the events from the CRM, process these events, and store the data in our Data Lakehouse stored on S3. The tech stack for this project included the Serverless framework, AWS Lambda, Flask, and S3.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I started by setting up a Flask application to create the webhook for listening to CRM events. The Serverless framework was used to enable the application&#39;s deployment and management without having to worry about the server infrastructure. When an event was detected, an AWS Lambda function was triggered to process the event data. The processed data was then stored in S3, our data lakehouse, making it accessible for further analysis and real-time insights.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: With the successful implementation of this project, the Sales team was able to get real-time insights from the CRM events. This has greatly improved their decision-making process, as they now have access to the latest data when they need it. The use of serverless technologies and AWS services has made this solution highly scalable and efficient, ensuring we can handle the growing needs of our Sales team.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 110.67px;"><img alt="" src="images/image4.png" style="width: 604.72px; height: 110.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Tech Stack:</span></p><ul class="c3 lst-kix_le5gy5easr5s-0 start"><li class="c1 li-bullet-0"><span class="c0">AWS</span></li><li class="c1 li-bullet-0"><span class="c0">Python</span></li><li class="c1 li-bullet-0"><span class="c0">AWS Lambda</span></li><li class="c1 li-bullet-0"><span class="c0">AWS S3</span></li><li class="c1 li-bullet-0"><span class="c0">Athena (Query Engine)</span></li><li class="c1 li-bullet-0"><span class="c0">Flask</span></li></ul><h2 class="c9" id="h.cn98i577yp3p"><span class="c10">Getting data from Twitter - parsing hashtags</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: As part of an academic project, I was tasked with gathering data from Twitter, focusing on specific hashtags. The goal was to gain insights about these hashtags and also to put my knowledge of Apache Nifi and StreamSets to the test.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to design and implement a data pipeline to extract tweets containing the target hashtags, process this data, and generate meaningful insights. The project would involve leveraging Apache Nifi and StreamSets tools.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I initiated the project by setting up data extraction from Twitter using Apache Nifi. I carefully configured the tool to focus on tweets containing our specific hashtags of interest. Then, I used StreamSets to process the incoming data stream and prepare it for analysis. Given that this was an academic project, I spent considerable time exploring the functionalities and capabilities of both Apache Nifi and StreamSets, focusing on both efficient data extraction and data processing.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: Despite being an academic exercise, the project offered significant learning experiences. I was able to successfully gather data from Twitter regarding specific hashtags and process it for insightful analysis. In addition, the project enabled me to gain practical experience and deepen my understanding of Apache Nifi and StreamSets. Ultimately, this project enhanced my skills in using these tools, setting a strong foundation for future data pipeline projects.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 200.00px;"><img alt="" src="images/image18.png" style="width: 604.72px; height: 200.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Apache NiFi</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.00px solid #1a1a1a; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 293.33px;"><img alt="" src="images/image14.png" style="width: 604.72px; height: 293.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">StreamSets</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.00px solid #1a1a1a; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 218.67px;"><img alt="" src="images/image10.png" style="width: 604.72px; height: 218.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">WorldCloud using Twitter Data</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.00px solid #1a1a1a; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 329.33px;"><img alt="" src="images/image5.png" style="width: 604.72px; height: 329.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.2e5akskz5mef"><span class="c10">Design and Implement the Data Lakehouse</span></h2><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Situation</span><span class="c0">: The company needed a robust data platform, relying only on a NoSQL database for analytics, which needed to be improved for promoting a data-driven culture and facilitating data consumption by data scientists and analysts. The necessity to incorporate more data sources, including data from blockchain, lending protocols, and cryptocurrency platforms, while reducing costs, underscored the need for a comprehensive solution.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My role was to design and implement a Data Lakehouse Platform to meet these requirements. This involved deciding on the open table format to be used, designing the architecture, and justifying the need for a data lakehouse to the team.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: To start, I conducted several proof-of-concept tests to decide on the open table format that best suited our needs. Following that, I designed the architecture of the Data Lakehouse platform, ensuring it could accommodate our current data sources and scale for future additions. I presented the design to the team, explaining the advantages of having a Data Lakehouse and how it would facilitate data access for data scientists and analysts. I then proceeded to implement the platform, ensuring the smooth integration of data from various sources, including blockchain, lending protocols, and cryptocurrency platforms.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: As a result of this project, we were able to establish a functional Data Lakehouse platform, enabling a data-driven culture within the company. The platform facilitated easy data access for data scientists and analysts, promoting more efficient and informed decision-making processes. Furthermore, we successfully incorporated data from various news sources while keeping costs low. The project significantly upgraded our data infrastructure, allowing for scalable and cost-effective data operations.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 322.67px;"><img alt="" src="images/image9.png" style="width: 604.72px; height: 322.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c9" id="h.nh80iby6zwf6"><span class="c10">Data Ingestion Wallet Transactions</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: Our organization required comprehensive transaction data from the Ethereum blockchain for various analyses. The available API from Etherscan was not providing data at the speed we required, hence we needed an in-house solution. Additionally, we wanted to apply data modeling to identify different types of transactions and create a balance summary for different wallets and coins.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to design and implement a solution for extracting and processing Ethereum blockchain data using Python, AWS ECS, Athena, EMR Serverless, S3, and Airflow. The project also involved data modeling and the creation of several aggregated tables.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I started by setting up a Python-based data extraction system on AWS ECS to gather Ethereum blockchain data. For data processing and analysis, I used Athena and EMR Serverless. The data was then stored in S3, which served as our data lake. I implemented different data modeling techniques to categorize the transactions into normal, internal, and erc20 types. Furthermore, I created aggregated tables to represent coin balances and wallet balances based on the processed data. I used Airflow to orchestrate and schedule the various data processing and ETL tasks.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: This project led to significant improvements in the speed and accuracy of our data collection and analysis related to Ethereum blockchain transactions. By bypassing the limitations of the Etherscan API, we could get real-time data at the pace we required. The data modeling provided us with a detailed view of different transaction types, and the aggregated tables ensured we had quick access to wallet and coin balances. This greatly enhanced our ability to analyze and understand blockchain transactions, leading to more informed decision-making processes within our organization.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 265.33px;"><img alt="" src="images/image2.png" style="width: 604.72px; height: 265.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 241.33px;"><img alt="" src="images/image8.png" style="width: 604.72px; height: 241.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Complete Data Architecture</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 535.61px; height: 236.00px;"><img alt="" src="images/image19.png" style="width: 605.00px; height: 236.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">This project was the one in which we utilized our data lakehouse platform the most. As such, we had to adhere to numerous best practices to ensure low costs, high performance, and the availability of data for the data science team.</span></p><p class="c2"><span class="c0">Best Practices that it was applied in the data lakehouse architecture:</span></p><ul class="c3 lst-kix_xlu6ijxiwzdn-0 start"><li class="c1 li-bullet-0"><span class="c0">Partition the data (e.g year-mont, year-mont-day, hash_partition)</span></li><li class="c1 li-bullet-0"><span class="c0">Optimize file sizes to avoid many small files (WIP)</span></li><li class="c1 li-bullet-0"><span class="c0">CTAS is helping us to create tables with a good layout of files</span></li><li class="c1 li-bullet-0"><span class="c0">Selecting just the columns that we need in the query (Athena)</span></li><li class="c1 li-bullet-0"><span class="c0">Save data as parquet file and use snappy as a compression</span></li><li class="c1 li-bullet-0"><span class="c0">Optimize joins (large table on the left side)</span></li></ul><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">This project also presented a great opportunity for me to learn more about blockchain data because I had to deploy an Ethereum Node. To do this, I had to test different clients to determine the best fit for our needs. In this case, we chose Erigon as the client because we needed to have an archive node. All data from traces was necessary for us. Currently, we are using it just to get internal transactions, but the future goal is also to decode and identify events for each transaction (e.g., TransferEvent from AAVE or Compound).</span></p><h2 class="c9" id="h.ei592a6ppprz"><span class="c10">Data Ingestion Polygon Subgrap (The Graph) - DeFi Events</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: We needed to fetch data from The Graph, specifically DeFi data from the Polygon subgraph. The data of interest were events generated by lending protocols such as Aave, Compound, and Avalanche.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to set up a reliable data extraction and processing pipeline to get this specific DeFi data from the Polygon subgraph on The Graph.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I started the project by configuring the extraction parameters to focus on the DeFi events of interest from lending protocols like Aave, Compound, and Avalanche. I utilized appropriate tools and scripts to ensure the extraction was accurate and efficient. The data was then processed and transformed as per our analytical needs.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: Through the successful implementation of this project, we were able to effectively fetch and process relevant DeFi data from the Polygon subgraph. This greatly enriched our data resources and enabled us to conduct more detailed and specific analyses on the events of the various lending protocols. The insights gathered from these analyses significantly contributed to our understanding of the DeFi landscape and facilitated more informed decision-making.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 189.33px;"><img alt="" src="images/image3.png" style="width: 604.72px; height: 189.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.4usc2hr4wxvl"><span class="c10">Getting Unique EOA Address from DeFi Events</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: We had a project that required obtaining unique EOA (Externally Owned Accounts) addresses based on all DeFi events from AAVE V1, V2, and Compound. The aim was to construct a table that included the wallet address, the timestamp of the first and last transactions, and the last protocol the wallet interacted with, and restricted to one row per wallet. This task was particularly challenging due to the need for upserts in our data lakehouse, a functionality typically not available.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to design and implement a data extraction, processing, and storage pipeline to fulfill this unique requirement. The solution had to utilize Apache Hudi as an open table format, given its ability to handle upserts effectively in a data lakehouse context.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I began by setting up the data extraction mechanism to fetch the specific DeFi events from AAVE V1, V2, and Compound. The extracted data were processed to derive unique EOA addresses, the timestamp of the first and last transactions, and the last protocol each wallet interacted with. I then used Apache Hudi to store this data in the data lakehouse, leveraging its upsert functionality to manage data updates and insertions effectively.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: The successful completion of this project led to the creation of a table that served our specific needs, enabling us to track unique EOA addresses based on DeFi events efficiently. Using Apache Hudi proved vital for handling upserts in our data lakehouse, thereby making it possible to update and insert data in an efficient and practical manner. This project enriched our data resources and boosted our ability to conduct detailed analyses of DeFi events and wallet interactions.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c12 c6">Data Pipeline:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 200.00px;"><img alt="" src="images/image15.png" style="width: 604.72px; height: 200.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c12 c6">Solutions Architecture:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 233.33px;"><img alt="" src="images/image13.png" style="width: 604.72px; height: 233.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><h2 class="c9" id="h.ms25skxrxpm7"><span class="c10">Deploy MWAA in a private network</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: We had an issue with the current configuration of our Managed Workflows for Apache Airflow (MWAA) provided by AWS. It was set up using less-than-optimal networking practices, leading to high data traffic costs and security concerns. It was accessible outside a private network, which was not in line with our security guidelines.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to reconfigure the MWAA to operate within a private network accessible only through a VPN, aiming to increase security and reduce costs associated with data traffic.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I started the project by assessing the current MWAA setup and identifying the key areas causing high data traffic costs and potential security vulnerabilities. The main issue was the use of a NAT gateway which was contributing to the high costs and wasn&#39;t necessary for our use case. I then reconfigured the MWAA to operate within a private network and replaced the NAT gateway with an Internet Gateway, which was adequate for our needs.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: This project led to a significant reduction in data traffic costs by nearly 90%. We increased the security of our MWAA by ensuring it was only accessible through a VPN, reducing the risk of unauthorized access. The implementation of the Internet Gateway instead of the NAT gateway served our needs adequately, proving that more complex and expensive solutions are not always better. This project demonstrated the importance of </span></p><p class="c2"><span class="c0">optimal configuration in managing costs and maintaining high-security levels.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c0">MWAA Architecture</span></p><h1 class="c11" id="h.cq124pjzs9lk"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 392.00px;"><img alt="" src="images/image17.png" style="width: 604.72px; height: 392.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><h1 class="c11 c13" id="h.p4axn65zlztp"><span class="c5"></span></h1><h2 class="c9" id="h.z8iqclfgz5d2"><span class="c10">Observability Alerts for Lambdas</span></h2><p class="c2"><span class="c6">Situation</span><span class="c0">: We had more than 20 AWS Lambda functions operating as part of our infrastructure. However, we lacked a system to notify us on Slack whenever any of these Lambda functions failed, creating a gap in our monitoring and observability practices.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Task</span><span class="c0">: My task was to create an observability stack that would alert us on Slack whenever a Lambda function failed. This new system would increase our visibility into system operations and allow us to respond more quickly to failures.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Action</span><span class="c0">: I started the project by understanding the existing Lambda functions and identifying the data necessary for useful alerts. Then, I utilized Python to script a solution that would track Lambda function execution and trigger an alert message to be sent to Slack if any function failed. This message included details about the function and the nature of the failure to assist with troubleshooting.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">Result</span><span class="c0">: The successful implementation of this project resulted in significantly improved monitoring of our Lambda functions. The real-time Slack alerts allowed our team to immediately respond to any function failures, reducing potential downtime and improving system reliability. This observability stack has become a crucial part of our infrastructure management, improving our ability to maintain consistent service quality.</span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 422.67px;"><img alt="" src="images/image6.png" style="width: 604.72px; height: 422.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c6 c12">Workflow</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 604.72px; height: 206.67px;"><img alt="" src="images/image12.png" style="width: 604.72px; height: 206.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><h1 class="c11" id="h.9hxvajqct9js"><span class="c5">More info</span></h1><p class="c2"><span class="c0">More about the technologies I use as a Data Engineer:</span></p><p class="c2 c4"><span class="c0"></span></p><ul class="c3 lst-kix_c4cd17rhy6dz-0 start"><li class="c1 li-bullet-0"><span class="c0">AWS Cloud</span></li><li class="c1 li-bullet-0"><span class="c0">AWS Athena (Query Engine for data lake and data lakehouse)</span></li><li class="c1 li-bullet-0"><span class="c0">AWS S3</span></li><li class="c1 li-bullet-0"><span class="c0">AWS Glue</span></li><li class="c1 li-bullet-0"><span class="c0">AWS ECS</span></li><li class="c1 li-bullet-0"><span class="c0">AWS EFS</span></li><li class="c1 li-bullet-0"><span class="c0">AWS RDS</span></li><li class="c1 li-bullet-0"><span class="c0">AWS DocumentDB</span></li><li class="c1 li-bullet-0"><span class="c0">AWS DynamoDB</span></li><li class="c1 li-bullet-0"><span class="c0">Databricks</span></li><li class="c1 li-bullet-0"><span class="c0">Apache Spark (Pyspark and SparkSQL)</span></li><li class="c1 li-bullet-0"><span class="c0">AWS EMR to run Pyspark</span></li><li class="c1 li-bullet-0"><span class="c0">DBT</span></li><li class="c1 li-bullet-0"><span class="c0">Docker</span></li><li class="c1 li-bullet-0"><span class="c0">Open Table Formats for Data Lakehouse:</span></li></ul><ul class="c3 lst-kix_c4cd17rhy6dz-1 start"><li class="c2 c16 li-bullet-0"><span class="c0">Apache Hudi</span></li><li class="c2 c16 li-bullet-0"><span class="c0">Apache Iceberg</span></li><li class="c2 c16 li-bullet-0"><span class="c0">Delta Lake</span></li></ul><ul class="c3 lst-kix_c4cd17rhy6dz-0"><li class="c1 li-bullet-0"><span class="c0">Airflow (Deployed using AWS ECS or using MWAA, also with K8s)</span></li><li class="c1 li-bullet-0"><span class="c0">Python</span></li><li class="c1 li-bullet-0"><span class="c0">SQL</span></li><li class="c1 li-bullet-0"><span class="c0">Terraform</span></li><li class="c1 li-bullet-0"><span>Airbyte</span></li></ul><p class="c2 c4"><span class="c0"></span></p><h1 class="c11" id="h.u07yujqdxrin"><span class="c5">Conclusion</span></h1><p class="c2"><span class="c0">Here&#39;s a bit about the projects I&#39;ve worked on throughout my career as a Data Engineer. I&#39;m always striving to generate business value using various technologies. Ultimately, what&#39;s important is helping the business to grow and achieve its goals, irrespective of the technology we are using. If we are achieving the goal, that&#39;s what should be prioritized in every project, especially in data projects, because data can significantly impact the business. </span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c6">My LinkedIn profile: </span><span class="c6 c15"><a class="c7" href="https://www.google.com/url?q=https://www.linkedin.com/in/jhon-lucas/&amp;sa=D&amp;source=editors&amp;ust=1694139887283013&amp;usg=AOvVaw2CNGzzoFMGDiKLQHeq3Vwr">https://www.linkedin.com/in/jhon-lucas/</a></span></p><p class="c2"><span class="c6">My GitHub: </span><span class="c15 c6"><a class="c7" href="https://www.google.com/url?q=https://github.com/l-jhon&amp;sa=D&amp;source=editors&amp;ust=1694139887283554&amp;usg=AOvVaw033LjatM-DuGQzLoNlfoVR">https://github.com/l-jhon</a></span></p><h1 class="c11" id="h.5d6e6fhbbf2z"><span class="c5">Resources</span></h1><p class="c2"><span class="c15"><a class="c7" href="https://www.google.com/url?q=https://docs.aws.amazon.com/athena/latest/ug/what-is.html&amp;sa=D&amp;source=editors&amp;ust=1694139887284091&amp;usg=AOvVaw0rrVMfPohMrWTweBJCULKi">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></span></p><p class="c2"><span class="c15"><a class="c7" href="https://www.google.com/url?q=https://airflow.apache.org/&amp;sa=D&amp;source=editors&amp;ust=1694139887284433&amp;usg=AOvVaw2kWsZWCvmw2DaRysVH9Rq8">https://airflow.apache.org/</a></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c15"><a class="c7" href="https://www.google.com/url?q=https://www.reddit.com/r/ethstaker/comments/pdpj62/for_anyone_who_hasnt_checked_out_erigon_yet_i/&amp;sa=D&amp;source=editors&amp;ust=1694139887284935&amp;usg=AOvVaw0npOwx5cVOOJwtPlcSuFwO">https://www.reddit.com/r/ethstaker/comments/pdpj62/for_anyone_who_hasnt_checked_out_erigon_yet_i/</a></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2"><span class="c15"><a class="c7" href="https://www.google.com/url?q=https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi&amp;sa=D&amp;source=editors&amp;ust=1694139887285400&amp;usg=AOvVaw2nZAJfq5rBdZffBZN5KQ7O">https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi</a></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p><p class="c2 c4"><span class="c0"></span></p></body></html>